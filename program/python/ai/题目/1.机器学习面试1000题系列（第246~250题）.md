BAT题库 | 机器学习面试1000题系列（第246~250题） - 七月在线实验室 - CSDN博客 https://blog.csdn.net/T7SFOKzorD1JAYMSFk4/article/details/78758374

246.对于神经网络的说法, 下面正确的是 : 

1. 增加神经网络层数, 可能会增加测试数据集的分类错误率
2. 减少神经网络层数, 总是能减小测试数据集的分类错误率
3. 增加神经网络层数, 总是能减小训练数据集的分类错误率

A. 1

B. 1 和 3

C. 1 和 2

D. 2

答案: A

深度神经网络的成功, 已经证明, 增加神经网络层数, 可以增加模型范化能力, 即, 训练数据集和测试数据集都表现得更好. 但更多的层数, 也不一定能保证有更好的表现（https://arxiv.org/pdf/1512.03385v1.pdf）. 所以, 不能绝对地说层数多的好坏, 只能选A

 

247.假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？

A. 设C=1

B. 设C=0

C. 设C=无穷大

D. 以上都不对

答案: C

C无穷大保证了所有的线性不可分都是可以忍受的.

 

248.训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类:

A. 正确

B. 错误

答案: A

SVM模型中, 真正影响决策边界的是支持向量

 

249.以下哪些算法, 可以用神经网络去构造: 

1. KNN
2. 线性回归
3. 对数几率回归

A. 1和 2

B. 2 和 3

C. 1, 2 和 3

D. 以上都不是

答案: B

1. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙
2. 最简单的神经网络, 感知器, 其实就是线性回归的训练
3. 我们可以用一层的神经网络构造对数几率回归

 

250.请选择下面可以应用隐马尔科夫(HMM)模型的选项: 

A. 基因序列数据集

B. 电影浏览数据集

C. 股票市场数据集

D. 所有以上

答案: D

只要是和时间序列问题有关的 , 都可以试试HMM
--------------------- 
作者：七月在线实验室 
来源：CSDN 
原文：https://blog.csdn.net/T7SFOKzorD1JAYMSFk4/article/details/78758374 
版权声明：本文为博主原创文章，转载请附上博文链接！